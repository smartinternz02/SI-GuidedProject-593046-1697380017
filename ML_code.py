# -*- coding: utf-8 -*-
"""Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gST6RyLLuit3_LeoSzrizA9I8CYUWAiL
"""

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import pickle as pkl
import numpy as np
import random
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from xgboost import XGBClassifier
from sklearn.preprocessing import Normalizer
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix

data = pd.read_csv("dataset.csv")
data.head()

data.shape

data.info()

data.isnull().sum()

label_map={}
for i in data.columns:
  if str(data[i].dtype) == 'object':
      temp={}
      cats=data[i].unique()
      for index in range(len(cats)):
           temp[cats[index]]=index
      label_map[i]=temp
      #Labeling
      data[i]=data[i].map(temp)
label_map

c=0
plt.figure(figsize=(18, 10))
for i in data.drop(columns=[
                            'Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender','Reached.on.Time_Y.N','ID'
                           ]).columns:
      if str(data[i].dtype)=="object ":
            continue
      plt.subplot(2, 3, c+1)
      plt.boxplot(data[i])
      plt.title(i)
      c+=1
plt.show()

def check_outliers (arr):
        Q1= np.percentile(arr, 25, interpolation = 'midpoint')
        Q3= np.percentile(arr, 75, interpolation = 'midpoint')
        IQR = Q3 - Q1

        #Above Upper bound
        upper=Q3+1.5*IQR
        upper_array=np.array(arr>=upper)
        print(' '*3,len(upper_array[upper_array == True]), 'are over the upper bound:',upper)
        #BeLow Lower bound
        lower=Q1-1.5*IQR
        lower_array=np.array(arr<=lower)
        print(' '*3,len(lower_array[lower_array == True]), 'are less than the lower bound:', lower, '\n')
for i in data.drop(columns=[
                            'Warehouse_block', 'Mode_of_Shipment', 'Product_importance', 'Gender', 'Reached.on.Time_Y.N','ID'
                           ]).columns:
        if str(data[i].dtype)=='object':
            continue
        print(i)
        check_outliers(data[i])

data.describe(include='all')

for column in data.columns:
         random_color = "#{:02x}{:02x}{:02x}".format(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))
         plt.figure(figsize=(8, 4))
         plt.hist(data[column], bins=50, color=random_color)
         plt.title(f'{column}')  # Set the title
         plt.xlabel(column)  # Set the x-axis label
         plt.ylabel('Count')  # Set the y-axis label
         plt.show()  # Show the plot

target_column = 'Reached.on.Time_Y.N'
column_to_skip = 'ID'
for column in data.columns:
    if column != target_column and column != column_to_skip:
        plt.figure(figsize=(8, 6))
        plt.scatter(data[column], data[target_column])
        plt.title(f'Scatter Plot of {column} vs. {target_column}')
        plt.xlabel(column)
        plt.ylabel(target_column)
        plt.show()

correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

x = data[['Discount_offered','Weight_in_gms']]
y = data['Reached.on.Time_Y.N']

x_train, x_test, y_train, y_test = train_test_split(x,y,
                                        random_state=1234,test_size = 0.10,
                                        shuffle=True
                                        )
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.preprocessing import StandardScaler

def models_eval_mm(x_train, y_train, x_test, y_test):

    # Logistic Regression
    lg = LogisticRegression(random_state=1234)
    lg.fit(x_train, y_train)
    print("--Logistic Regression--")
    print("Train Score:", lg.score(x_train, y_train))
    print("Test Score:", lg.score(x_test, y_test))
    print()

    # Logistic Regression with Cross-Validation
    lcv = LogisticRegression(random_state=1234)
    lcv.fit(x_train, y_train)
    print("--Logistic Regression CV--")
    print("Train Score:", lcv.score(x_train, y_train))
    print("Test Score:", lcv.score(x_test, y_test))
    print()

    # XGBoost
    xgb = XGBClassifier(random_state=1234)
    xgb.fit(x_train, y_train)
    print("--XGBoost--")
    print("Train Score:", xgb.score(x_train, y_train))
    print("Test Score:", xgb.score(x_test, y_test))
    print()

    # Ridge Classifier
    rg = RidgeClassifier(random_state=1234)
    rg.fit(x_train, y_train)
    print("--Ridge Classifier--")
    print("Train Score:", rg.score(x_train, y_train))
    print("Test Score:", rg.score(x_test, y_test))
    print()

    # K-Nearest Neighbors
    knn = KNeighborsClassifier()
    knn.fit(x_train, y_train)
    print("--KNN--")
    print("Train Score:", knn.score(x_train, y_train))
    print("Test Score:", knn.score(x_test, y_test))
    print()

    # Random Forest
    rf = RandomForestClassifier(random_state=1234)
    rf.fit(x_train, y_train)
    print("--Random Forest--")
    print("Train Score:", rf.score(x_train, y_train))
    print("Test Score:", rf.score(x_test, y_test))
    print()

    # Support Vector Machine (SVM)
    svc = svm.SVC(random_state=1234)
    svc.fit(x_train, y_train)
    print("--SVM Classifier--")
    print("Train Score:", svc.score(x_train, y_train))
    print("Test Score:", svc.score(x_test, y_test))
    print()

    return lg, lcv, xgb, rg, knn, rf, svc

lg,lcv,xgb,rg,knn,rf,svc =  models_eval_mm(x_train,y_train,x_test,y_test)

# Logistic Regression
lg_prediction = lg.predict(x_test.iloc[0].values.reshape(1, -1))
print("Logistic Regression Prediction:", lg_prediction)

# Logistic Regression with Cross-Validation
lcv_prediction = lcv.predict(x_test.iloc[0].values.reshape(1, -1))
print("Logistic Regression with Cross-Validation Prediction:", lcv_prediction)

# XGBoost
xgb_prediction = xgb.predict(x_test.iloc[0].values.reshape(1, -1))
print("XGBoost Prediction:", xgb_prediction)

# Ridge Classifier
rg_prediction = rg.predict(x_test.iloc[0].values.reshape(1, -1))
print("Ridge Classifier Prediction:", rg_prediction)

# K-Nearest Neighbors
knn_prediction = knn.predict(x_test.iloc[0].values.reshape(1, -1))
print("K-Nearest Neighbors Prediction:", knn_prediction)

# Random Forest
rf_prediction = rf.predict(x_test.iloc[0].values.reshape(1, -1))
print("Random Forest Prediction:", rf_prediction)

# Support Vector Machine (SVM)
svc_prediction = svc.predict(x_test.iloc[0].values.reshape(1, -1))
print("Support Vector Machine (SVM) Prediction:", svc_prediction)

def evaluate_model(name, model, x_test, y_test):
    y_pred = model.predict(x_test)
    result = []
    result.append(name)
    result.append(f"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")
    result.append(f"F1 Score: {f1_score(y_test, y_pred) * 100:.2f}%")
    result.append(f"Recall: {recall_score(y_test, y_pred) * 100:.2f}%")
    result.append(f"Precision: {precision_score(y_test, y_pred) * 100:.2f}%")
    return result

model_list = {
    "Logistic Regression": lg,
    "Logistic Regression CV": lcv,
    "XGBoost": xgb,
    "Ridge Classifier": rg,
    "KNN": knn,
    "Random Forest": rf,
    "Support Vector Classifier": svc
}

model_eval_info = []
for model_name, model in model_list.items():
    model_eval_info.append(evaluate_model(model_name, model, x_test, y_test))

import pandas as pd

model_eval_info = pd.DataFrame(model_eval_info, columns=["Name", "Accuracy", "F1 Score", "Recall", "Precision"])
model_eval_info.to_csv("model_eval.csv")

model_eval_info

import pickle

filename="bestmodel.pkl"
pickle.dump(knn,open(filename,'wb'))

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()

filename2="Scaler.pkl"
pickle.dump(scaler,open(filename2,'wb'))